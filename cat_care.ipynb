{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca16cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 308926 characters from PDF.\n",
      "Total chunks: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30d001d379d4673804f71904b39cd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (269, 384)\n",
      "FAISS index contains 269 vectors.\n",
      "\n",
      "Top chunks relevant to query:\n",
      "\n",
      "Chunk 1:\n",
      "You will \n",
      "not find a fussy eater in a \n",
      "household where there is \n",
      "more than one cat. \n",
      "'' vA. \n",
      "Overfeeding \n",
      "Feed your cat two to three \n",
      "small meals a day, following the \n",
      "manufacturer’s recommendations. Do \n",
      "not give many snacks between meals. \n",
      "Do not \n",
      "give a cat \n",
      "too many \n",
      "scraps \n",
      "WATER AND MILK \n",
      "A cat gets most of the moisture that \n",
      "it requires from its food, and many \n",
      "felines seem to drink little. However, \n",
      "you should make sure that fresh water \n",
      "is available at all times. If a cat is fed \n",
      "dry foo ...\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2:\n",
      "Sometimes a cat supplements \n",
      "its diet by catching and eating \n",
      "small prey animals, but this \n",
      "does not mean that it is hungry \n",
      "or that you can prevent it from \n",
      "hunting by feeding it more. A \n",
      "cat hunts through instinct, and \n",
      "even a well-fed pet may catch \n",
      "mice given the opportunity. \n",
      "Protein is essential for the growth \n",
      "and repair of tissues and for the \n",
      "regulation of metabolic processes \n",
      "Essential fatty acids help give a \n",
      "sheen to a cat’s coat \n",
      "Vitamin A is \n",
      "essential for \n",
      "healthy eyes \n",
      "Calcium, a ...\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3:\n",
      "REDUCING WEIGHT \n",
      "Consult a vet to make sure that \n",
      "your cat’s excess weight is not \n",
      "due to a medical problem. Cut \n",
      "down the cat’s calorie intake by \n",
      "reducing the amount of food \n",
      "given under veterinaiy \n",
      "guidance. You can obtain a \n",
      "low-calorie diet from your \n",
      "vet or from a store that is \n",
      "especially formulated for \n",
      "feline weight loss. \n",
      "Obesity \n",
      "A very overweight cat has \n",
      "a shorter life expectancy \n",
      "than other cats. \n",
      "58 \n",
      "A CAT’S DIETARY NEEDS \n",
      "THE NUTRITIONAL REQUIREMENTS OF AN ADULT CAT* \n",
      "Component \n",
      " ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# PDF to RAG-ready embeddings\n",
    "# -------------------------------\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Load PDF and extract text\n",
    "pdf_path = r\"C:\\Users\\user\\Downloads\\dokumen.pub_complete-cat-care-manual-the-essential-practical-guide-to-all-aspects-of-caring-for-your-cat-illustrated-0756617421-9780756617424.pdf\"  # Replace with your PDF\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "full_text = \"\"\n",
    "for page in doc:\n",
    "    full_text += page.get_text()\n",
    "\n",
    "print(f\"Extracted {len(full_text)} characters from PDF.\")\n",
    "\n",
    "# 2️⃣ Split text into chunks (to fit LLM input limits)\n",
    "def chunk_text(text, max_words=200):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk.split()) + len(sentence.split()) <= max_words:\n",
    "            chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \". \"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(full_text)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# 3️⃣ Generate embeddings for each chunk\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embed_model.encode(chunks)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# 4️⃣ Store embeddings in FAISS vector database\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index contains {index.ntotal} vectors.\")\n",
    "\n",
    "# 5️⃣ Example: retrieve top 3 relevant chunks for a query\n",
    "query = \"What feed is best for cat with low milk production?\"\n",
    "query_emb = embed_model.encode([query]).astype(\"float32\")\n",
    "\n",
    "k = 3  # top 3\n",
    "distances, indices = index.search(query_emb, k)\n",
    "\n",
    "print(\"\\nTop chunks relevant to query:\\n\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunks[idx][:500], \"...\")  # show first 500 chars\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7eab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b349e467dd2d487abb2e1a6c23e6c066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " 57 FEEDING EQUIPMENT\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Combine top chunks into a single prompt\n",
    "context = \" \".join([chunks[idx] for idx in indices[0]])\n",
    "prompt = f\"summarize or answer the question based on context: {context} Question: {query}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "output_ids = model.generate(inputs[\"input_ids\"], max_length=150)\n",
    "answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nAnswer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e43ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text...\n",
      "Warning: page 2 no meaningful text\n",
      "Warning: page 3 no meaningful text\n",
      "Warning: page 4 no meaningful text\n",
      "Warning: page 6 no meaningful text\n",
      "Warning: page 14 no meaningful text\n",
      "Warning: page 82 no meaningful text\n",
      "Warning: page 92 no meaningful text\n",
      "Warning: page 130 no meaningful text\n",
      "Warning: page 197 no meaningful text\n",
      "Warning: page 199 no meaningful text\n",
      "Extracted 309,116 characters\n",
      "Cleaned: 296,856 characters\n",
      "\n",
      "Counting chunks (sanity check)...\n",
      "Expected total chunks: 707  ← should be ~400–800 for this book\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f509dac26714c678e852051129468b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting low-memory encoding + streaming save...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:02<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 64 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:04<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:07<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 192 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:09<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 256 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:12<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 320 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:15<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 384 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:18<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 448 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:20<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 512 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:22<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 576 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 0/707 [00:25<?, ?chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 640 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding chunks:   0%|          | 3/707 [00:29<1:54:39,  9.77s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 704 chunks\n",
      "Processed final partial batch (707 total chunks)\n",
      "\n",
      "Total chunks written: 707\n",
      "Embeddings saved: cat_care_manual_models\\embeddings.npy  shape = (707, 384)\n",
      "Size on disk ≈ 1.1 MB\n",
      "\n",
      "Done! Files saved in:\n",
      "c:\\Users\\user\\ML_Projects\\RAG_BASED_LLM\\cat_care_manual_models\n",
      "   • embeddings.npy\n",
      "   • chunks.txt\n",
      "   • info.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== PDF → Embeddings + Chunks.txt (EXTREME low-memory version - FIXED) ======\n",
    "# For ~1–4 GB RAM environments\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF - pip install pymupdf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  # pip install tqdm (optional but recommended)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'Page \\d+( of \\d+)?', '', text, flags=re.I)\n",
    "    text = re.sub(r'^\\s*[\\dIVXLCDM]+\\s*$', '', text, flags=re.M)\n",
    "    text = text.replace('•', ' - ').replace('', ' - ')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_generator(text: str, max_chars: int = 500, overlap: int = 80):\n",
    "    \"\"\"\n",
    "    Safe, forward-only chunking. No infinite loop risk.\n",
    "    Skips tiny fragments from pdfplumber-style extraction noise.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return\n",
    "    n = len(text)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + max_chars, n)\n",
    "        chunk = text[start:end].strip()\n",
    "        if len(chunk) >= 60:  # skip very short junk\n",
    "            yield chunk\n",
    "        # Always advance by fixed amount\n",
    "        start += max_chars - overlap\n",
    "        if (max_chars - overlap) <= 0:\n",
    "            break\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# MAIN PIPELINE\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "pdf_path = r\"C:\\Users\\user\\Downloads\\dokumen.pub_complete-cat-care-manual-the-essential-practical-guide-to-all-aspects-of-caring-for-your-cat-illustrated-0756617421-9780756617424.pdf\"\n",
    "\n",
    "if not os.path.isfile(pdf_path):\n",
    "    print(f\"File not found: {pdf_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# 1. Extract text (using faster PyMuPDF)\n",
    "print(\"Extracting text...\")\n",
    "full_text = \"\"\n",
    "try:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        txt = page.get_text(\"text\")\n",
    "        if txt.strip():\n",
    "            full_text += txt + \"\\n\"\n",
    "        else:\n",
    "            print(f\"Warning: page {page_num} no meaningful text\")\n",
    "    doc.close()\n",
    "except Exception as e:\n",
    "    print(f\"PDF error: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Extracted {len(full_text):,} characters\")\n",
    "\n",
    "cleaned_text = clean_text(full_text)\n",
    "print(f\"Cleaned: {len(cleaned_text):,} characters\")\n",
    "\n",
    "# 2. Quick sanity check: how many chunks will we actually get?\n",
    "print(\"\\nCounting chunks (sanity check)...\")\n",
    "chunk_count = sum(1 for _ in chunk_generator(cleaned_text))\n",
    "print(f\"Expected total chunks: {chunk_count}  ← should be ~400–800 for this book\")\n",
    "\n",
    "if chunk_count > 2000:\n",
    "    print(\"WARNING: Chunk count too high → something still wrong with text. Aborting.\")\n",
    "    exit(1)\n",
    "\n",
    "# 3. Load embedding model\n",
    "print(\"\\nLoading model...\")\n",
    "try:\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    print(f\"Model load failed: {e}\")\n",
    "    print(\"Try: pip install --upgrade sentence-transformers torch transformers\")\n",
    "    exit(1)\n",
    "\n",
    "# 4. Prepare output\n",
    "save_dir = \"cat_care_manual_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "embed_path = os.path.join(save_dir, \"embeddings.npy\")\n",
    "chunks_path = os.path.join(save_dir, \"chunks.txt\")\n",
    "\n",
    "print(\"\\nStarting low-memory encoding + streaming save...\")\n",
    "\n",
    "all_emb_parts = []\n",
    "batch = []\n",
    "BATCH_SIZE = 64           # Increased — safe with low chunk count\n",
    "chunk_idx = 0\n",
    "\n",
    "pbar = tqdm(desc=\"Encoding chunks\", unit=\"chunk\", total=chunk_count)\n",
    "\n",
    "with open(chunks_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "    for chunk in chunk_generator(cleaned_text, max_chars=500, overlap=80):\n",
    "        chunk_idx += 1\n",
    "        batch.append(chunk)\n",
    "\n",
    "        # Stream write chunk\n",
    "        txt_file.write(f\"─ Chunk {chunk_idx} ({len(chunk)} chars) ─{'─'*40}\\n\")\n",
    "        txt_file.write(chunk + \"\\n\\n\")\n",
    "\n",
    "        if len(batch) >= BATCH_SIZE:\n",
    "            try:\n",
    "                batch_emb = embed_model.encode(\n",
    "                    batch,\n",
    "                    batch_size=len(batch),\n",
    "                    convert_to_numpy=True,\n",
    "                    normalize_embeddings=True,\n",
    "                    show_progress_bar=False\n",
    "                )\n",
    "                all_emb_parts.append(batch_emb)\n",
    "                print(f\"Processed {chunk_idx} chunks\")\n",
    "            except MemoryError:\n",
    "                print(\"OOM → reduce BATCH_SIZE to 32 and retry\")\n",
    "                exit(1)\n",
    "            batch = []  # free memory\n",
    "            pbar.update(len(batch))  # wait — actually update after append\n",
    "\n",
    "    # Final partial batch\n",
    "    if batch:\n",
    "        batch_emb = embed_model.encode(\n",
    "            batch,\n",
    "            batch_size=len(batch),\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        all_emb_parts.append(batch_emb)\n",
    "        print(f\"Processed final partial batch ({chunk_idx} total chunks)\")\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(f\"\\nTotal chunks written: {chunk_idx}\")\n",
    "\n",
    "# 5. Save embeddings\n",
    "if all_emb_parts:\n",
    "    embeddings = np.vstack(all_emb_parts)\n",
    "    np.save(embed_path, embeddings)\n",
    "    print(f\"Embeddings saved: {embed_path}  shape = {embeddings.shape}\")\n",
    "    print(f\"Size on disk ≈ {embeddings.nbytes / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(\"No embeddings created (empty text?)\")\n",
    "    embeddings = np.array([])\n",
    "\n",
    "# 6. Info file\n",
    "info_path = os.path.join(save_dir, \"info.txt\")\n",
    "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"PDF: {os.path.basename(pdf_path)}\\n\")\n",
    "    f.write(f\"Chunks: {chunk_idx}\\n\")\n",
    "    f.write(f\"Embedding dim: {embeddings.shape[1] if embeddings.size > 0 else 'N/A'}\\n\")\n",
    "    f.write(\"Model: all-MiniLM-L6-v2\\n\")\n",
    "    f.write(\"Chunk size: ~500 chars + 80 overlap\\n\")\n",
    "    f.write(\"Extraction: PyMuPDF\\n\")\n",
    "\n",
    "print(f\"\\nDone! Files saved in:\\n{os.path.abspath(save_dir)}\")\n",
    "print(\"   • embeddings.npy\")\n",
    "print(\"   • chunks.txt\")\n",
    "print(\"   • info.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
